# Analytics App

This file includes code review of the application. You can find installation steps in INSTALL.md.

- Technologies used;
    * Varnish
    * Google Load Balancer
    * Google Compute Engine
    * Scala
    * Akka
    * BigQuery

- Project structure;
    * resources: Resource directory.
        * application.conf: Configuration file of the application.
    * bq: BigQuery related package.
        * BigQueryClient.scala: BigQuery client api mixin.
        * BqClient.scala: Base BigQuery client class.
        * DsOps.scala: Trait for Dataset operations.
        * QueryOps.scala: Trait for running queries against BigQuery
        * SQLTypes.scala: Case objects which represents SQL Types in BigQuery
        * TableOps.scala: Trait for Table operations.
    * connector: BigQuery connector package for request handlers.
        * BqConnector.scala: Trait for BigQuery connector to be used in actors.
    * constant: Package for application-wide constants.
        * ConfigStrings.scala: Values which represent strings in `application.conf`.
    * entities: Package for entities used by web server and request handlers.
        * RequestEntities.scala: Entities to represent requests.
        * ResponseEntities.scala: Entities to represent responses.
    * web: Web server related package.
        * RequestHandler.scala: Request handler powered by actors.
        * WebServer.scala: Web server which defines routing strategy.
    * Main.scala: Application bootstrapper.

## General Application Flow

1. POST
    * Query string is parsed into `PostRequestEntity`
    * For every request, an actor which is a request handler is created
    * `PostRequestEntity` is sent to an actor by using ask pattern
        * A request handler receives message which is `PostRequestEntity`
        * It converts timestamp to date in format 'yyyyMMdd'
        * This date format is going to be used as table name suffix
            * This means everyday interactions is going to be a different table in dataset
            * By doing this, it is eliminated that querying much larger table
        * Firstly, request handler will try to insert the data into table
        * If table does not exist, it creates the table and then insert the data
        * Actor uses tell pattern to send `ResponseEntity` back to web server
            * Actor stops itself
        * When web server gets back the response, it sends request to varnish to invalidate the cached data
2. GET
    * Query string is parsed into `GetRequestEntity`
    * For every request, an actor which is a request handler is created
    * `GetRequestEntiy` is sent to an actor by using ask pattern
        * A request handler receives message which is `GetRequestEntity`
        * It runs query against BigQuery and parse the result into the format
    * When web server gets back the response, it simply returns it

## General Script Flow

1. Between lines 1-12, a custom network is created, then a new subnet is assigned to the newly created network. Firewall rules are added to grant access between instances of this subnet.
2. Between lines 14-66, two web instances and two application instances are provisioned by using installation script, also static ip adresses are added to these instances.
3. Between lines 69-95, an external load balancer is provisioned and configured according to Google Cloud documentation.
4. Between lines 98-137, an internal load balancer is provisioned and configured according to Google Cloud documentation.

## General Concepts

To distribute the traffic on the cache servers and to provide high availability, the external load balancer is positioned at the front. For this project, two Varnish cache servers were used but the number can be increased if it is needed. An internal load balancer was added to distribute the traffic from the cache servers and to provide high availability. Two application servers were bound behind the internal load balancer. If it is needed, application servers can be increased. BigQuery is at the back.

### BigQuery Wrapper

This application uses BigQuery Java Client API which is written by Google with the following [builder design pattern](https://www.tutorialspoint.com/design_pattern/builder_pattern.htm). In Scala, we generally tend to use functional programming and avoid design patterns for imperative programming. To achieve this, we need a wrapper which does not fully encapsulate all properties of BigQuery Java Client API and gives a simple interface that lets developers skip underlying details. This is where `BigQueryClient` comes in.

`BigQueryClient` is just a mixin. This class is composed of several traits to provide useful APIs to developers. You can take a look at the documentation of `bq` package for further information. What we are doing with `bq` package is simply that; whenever we want to use BigQuery, we use `BigQueryClient` companion object to get instance of this class. `BigQueryClient` expects file path which is an authentication json file for your BigQuery and it simply returns a wrapper for BigQuery Java client API. If you do not want to use wrapper, all you need to do is calling `self` value of `BigQueryClient` instance, which is an instance of BigQuery Java Client.

When you take a look at source code of `BigQueryClient` class, you see nothing but just a class definition. All abilities that you can use with instance of a `BigQueryClient` comes from `DsOps`, `TableOps`, `QueryOps` traits. These traits support several operations on BigQuery. Instead of writing all of the operations in one trait, I have written them seperately. Because, in the future, someone may not want to use `TableOps` but just `QueryOps`. So, to keep practicing [interface segregation](https://en.wikipedia.org/wiki/Interface_segregation_principle), I have seperated them. Also, since I have used `BqClient` as base class and all traits depend on this class, in the future, anyone can easily replace `BigQueryClient` class with any other class which extends `BqClient`. This principle is also known as [Liskov Substitution](https://en.wikipedia.org/wiki/Liskov_substitution_principle).

`DsOps` trait's features has not been used in this project. I wrote it when I started to writing application. But then, I realized that it will not be a good practice to check whether dataset with given name exists or not, because I needed to write either `become/unbecome` pattern in actors or `if` expression (yes, `if` blocks are expressions in Scala :) ) which end up with adding up constant factors to complexity. At the end, I decided that it is better to use already created dataset for this application.

`TableOps` trait has overloaded method which name is `getTable`. You may wonder that what the reason is behind using `Either` monad instead of `Option` monad. You can not use `Option` monad with `Table` class because it is unreachable. To avoid check `null` value which is only instance of `Null`, I chose to use `Either`. If it was possible, I would prefer `Option` monad. You can find detailed information in documentations.

Additionally, you may wonder why I have used `SQLTypes` instead of directly using `LegacySqlTypeName` of BigQuery Java client API. Anyone can easily use `LegacySqlTypeName.valueOf` method to get `LegacySqlTypeName` instance. But to avoid exceptions because of typos, I have just created case object `STRING`. You can add additional case objects such as `BOOLEAN`, `TIMESTAMP`, etc.

###Â BigQuery Connector and Constants

`BqConnector` has been designed to be used with request handler which is simply an actor. Instead of directly using `BigQueryClient`, I have created bridge-like `BqConnector` which supports all functionalities which are going to be used by this application. Any additional functionalities can be added into `BqConnector`.

`application.conf` includes configuration parameters of the application. Whenever we change path of the configuration parameter, we need to change it also source code, sometimes repetitively. To avoid such a scenario, I have created `ConfigStrings` object which just holds configuration paths. Whenever you change path of the configuration, all you need to do is to changing it in `ConfigStrings`.

### Request and Response Entities

Web requests are coming to actors as `PostRequestEntity` and `GetRequestEntity`. Actors are sending responses as `ResponseEntity`. Request entities extend `ConvertTimestampToDate` trait. This trait has been designed to get date representation of timestamp. This date representation will become table name suffix. Also, `PostRequestEntity` implements `getAsJavaMap` method which returns values of case class in java hash map instance. The reason is that; BigQuery Java client API expects `java.util.Map` instance to be used as row content in insert operations.

### Web Server and Request Handlers

As mentioned, request handlers are just actors. Simply, application is creating an actor per request. Every single actor is request handler. According to request entity, it gets the job done. Companion object of request handler is provided for getting `Props` while creating actors. This is the recommended way to create actors in Akka.

WebServer includes routing and uses Akka Http server library. Companion class of web server expects actor system, execution context executor and ask timeout implicitly. I have used ask pattern instead of tell pattern because I wanted to wait for response of request handler.

#### Further Improvements

* Loggers can be added
* Exception handling mechanism in functional programming sense can be added at every layer
* Routes can be defined as traits